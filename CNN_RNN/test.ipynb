{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76ef253-599c-4467-b9ca-b3ce71777630",
   "metadata": {},
   "source": [
    "# 加载词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad62b8d-f834-4188-bc0b-39e32778c374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "vocab = json.load(open(\"./output/vocab.json\", encoding = 'utf-8'))\n",
    "itos = list(vocab['stoi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73891df7-6e85-4bfd-b25f-3c48ace53377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_decode(predictions_ids):\n",
    "    preds = []\n",
    "    for seq in predictions_ids:\n",
    "        preds.append(\" \".join([itos[id] for id in seq if id not in [0,1,2,3]]))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d3189a-fa6a-4cf7-b615-0aee6f2256b4",
   "metadata": {},
   "source": [
    "# 加载测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "957437c0-2380-4c70-b42d-b626303c6b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class IMCP_Test_Dataset(Dataset):\n",
    "  def __init__(self, image_path = \"./data/Val/\", summary_path = \"./data/val_data.json\"):\n",
    "    super().__init__()\n",
    "    self.data = json.load(open(summary_path, \"r\", encoding = 'utf-8'))\n",
    "    self.image_path = image_path\n",
    "    self.imgid2imgname = {entry['id']: entry['filename'] for entry in self.data['images']}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data['images'])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    entry = self.data['images'][index]\n",
    "    image_id = entry['id']\n",
    "    image_name = entry['filename']\n",
    "    image = Image.open(os.path.join(self.image_path, image_name)).convert('RGB')\n",
    "    caption = [self.data['annotations'][i]['segment_caption'] for i in range(len(self.data['annotations'])) if self.data['annotations'][i]['image_id'] == image_id]\n",
    "    return image, caption, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e244c-6111-49bb-95f7-37c21b709027",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036a6653-e844-4ff6-b1db-299a96378702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "max_seq_length = 256\n",
    "class IMCP_Collator:\n",
    "  def __init__(self, vocab, train = True, model = \"resnet101\"):\n",
    "    self.vocab = vocab['stoi']\n",
    "    self.bos_id = self.vocab['<bos>']\n",
    "    self.eos_id = self.vocab['<eos>']\n",
    "    self.pad_id = self.vocab['<pad>']\n",
    "    self.model = model\n",
    "    self.train = train\n",
    "\n",
    "  def tokenize_texts(self, captions):\n",
    "    raw_captions = [caption.split(\" \") for caption in captions]\n",
    "    truncated_captions = [s[:max_seq_length] for s in raw_captions]\n",
    "    max_len = max([len(c) for c in truncated_captions])\n",
    "\n",
    "    padded_captions = []\n",
    "    for c in truncated_captions:\n",
    "        c = [self.vocab[word] for word in c]\n",
    "        seq = [self.bos_id] + c + [self.eos_id] + [self.pad_id] * (max_len - len(c))\n",
    "        padded_captions.append(seq)\n",
    "\n",
    "    padded_captions = [torch.Tensor(seq).long() for seq in padded_captions]\n",
    "    padded_captions = pad_sequence(padded_captions, batch_first=True)\n",
    "    return padded_captions\n",
    "  \n",
    "  def resize_and_stack(self, images):\n",
    "    if self.model == \"resnet101\":\n",
    "      image_tensors = []\n",
    "      transform = transforms.Compose([\n",
    "          transforms.Resize((224, 224)),\n",
    "          transforms.ToTensor(),\n",
    "      ])\n",
    "      \n",
    "      for image in images:\n",
    "        img_tensor = transform(image)\n",
    "        image.close()\n",
    "        image_tensors.append(img_tensor)\n",
    "        \n",
    "      stacked = torch.stack(image_tensors)\n",
    "      return stacked\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    if self.train:\n",
    "      images = [example[0] for example in batch]\n",
    "      captions = [example[1] for example in batch]\n",
    "      return self.resize_and_stack(images), self.tokenize_texts(captions)\n",
    "    else:\n",
    "      images = [example[0] for example in batch]\n",
    "      captions = [example[1] for example in batch]\n",
    "      image_ids = [example[2] for example in batch]\n",
    "      return self.resize_and_stack(images), captions, image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5672ac4-e9ae-409c-8519-3017ce673f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = IMCP_Test_Dataset()\n",
    "collatorTest = IMCP_Collator(vocab, train = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 16, collate_fn = collatorTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6d239-1f0a-4448-a3c5-2b61e5c535db",
   "metadata": {},
   "source": [
    "# 读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a634d6-83f3-4c2b-a48a-7ff855cc21d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# 确定GPU是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载预训练ResNet101模型\n",
    "encoder = models.resnet101(pretrained=True).to(device)\n",
    "\n",
    "# 去除模型的最后一层\n",
    "modules = list(encoder.children())[:-1]\n",
    "encoder = nn.Sequential(*modules)\n",
    "\n",
    "# 冻结模型参数\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 定义LSTM解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feature_size, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + feature_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        captions = captions[:, :-1]\n",
    "        embeddings = self.embed(captions)\n",
    "        features = features.squeeze().unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
    "        embeddings = torch.cat((features, embeddings), 2)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, features, max_length):\n",
    "        batch_size = features.size(0)        \n",
    "        input = torch.empty(batch_size, 1, device = device, dtype = torch.long).fill_(collatorTest.bos_id)\n",
    "        track_complete_example = torch.zeros(batch_size)\n",
    "        hidden = None\n",
    "        cell = None\n",
    "        # 遍历所有timestep\n",
    "        for t in range(max_length - 1):\n",
    "            embeddings = self.embed(input)\n",
    "            features2 = features.squeeze(-1).squeeze(-1).unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
    "            embeddings = torch.cat((features2, embeddings), 2)\n",
    "            if t == 0:\n",
    "                _, (hidden, cell) = self.lstm(embeddings)\n",
    "            else:\n",
    "                _, (hidden, cell) = self.lstm(embeddings, (hidden, cell))\n",
    "\n",
    "            pred = torch.argmax(self.linear(hidden), axis = -1)\n",
    "            input = torch.cat([input, pred.permute(1, 0)], dim = 1)\n",
    "            where_end = torch.where(pred == collatorTest.pad_id)[0]\n",
    "            track_complete_example[where_end] = 1\n",
    "            if track_complete_example.eq(1).all():\n",
    "              print(\"Early break in generate!\")\n",
    "              break\n",
    "\n",
    "        return input\n",
    "\n",
    "\n",
    "# 定义超参数\n",
    "num_epochs = 2\n",
    "embed_size = 256\n",
    "feature_size = 2048\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab['stoi'].keys()) + 5\n",
    "num_layers = 1\n",
    "\n",
    "# 初始化解码器\n",
    "decoder = Decoder(feature_size, embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "decoder.load_state_dict(torch.load('./output/decoder.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7514-5bb0-4caa-b8d6-9063cf6ef6bd",
   "metadata": {},
   "source": [
    "# 生成caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4df56db-e3fa-4283-b089-0e0fbdf968ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early break in generate!\n"
     ]
    }
   ],
   "source": [
    "my_preds = []\n",
    "targets = []\n",
    "data = []\n",
    "decoder.eval()\n",
    "encoder.eval()\n",
    "for i, (images, captions, image_ids) in enumerate(test_dataloader):\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    features = encoder(images)\n",
    "    out = decoder.generate(features, 30)\n",
    "    preds = out.detach().cpu().numpy()\n",
    "    preds = batch_decode(preds)\n",
    "    my_preds.extend(preds)\n",
    "    for pred, image_id in zip(preds, image_ids):\n",
    "        data.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"caption\": pred\n",
    "        })\n",
    "    targets.extend(captions)\n",
    "import json\n",
    "with open(\"./output/result.json\", \"w\") as file:\n",
    "    json.dump(data, file, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
